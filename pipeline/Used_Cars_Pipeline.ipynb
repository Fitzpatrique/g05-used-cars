{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HamoyeHQ/g05-used-cars/blob/master/Preprocessing_and_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kfp\n",
      "  Downloading kfp-1.1.2.tar.gz (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from kfp) (5.3.1)\n",
      "Requirement already satisfied: google-cloud-storage>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.30.0)\n",
      "Collecting kubernetes<12.0.0,>=8.0.0\n",
      "  Downloading kubernetes-11.0.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 13.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-auth>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (1.23.0)\n",
      "Collecting requests_toolbelt>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from kfp) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.1b1\n",
      "  Downloading kfp-server-api-1.1.2rc1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 4.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp) (3.2.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from kfp) (0.8.7)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from kfp) (7.1.2)\n",
      "Collecting Deprecated\n",
      "  Downloading Deprecated-1.2.10-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting strip-hints\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Collecting docstring-parser>=0.7.3\n",
      "  Downloading docstring_parser-0.7.3.tar.gz (13 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting kfp-pipeline-spec<0.2.0,>=0.1.0\n",
      "  Downloading kfp_pipeline_spec-0.1.3.1-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.1.0)\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage>=1.13.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (0.57.0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (50.3.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.25.11)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (1.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2.24.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.7/site-packages (from kubernetes<12.0.0,>=8.0.0->kfp) (2020.11.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-auth>=1.6.1->kfp) (4.6)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (0.17.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (2.0.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema>=3.0.1->kfp) (20.3.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated->kfp) (1.12.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints->kfp) (0.35.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\" in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (1.0.0)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.22.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12.0.0,>=8.0.0->kfp) (3.1.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kubernetes<12.0.0,>=8.0.0->kfp) (2.10)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema>=3.0.1->kfp) (3.4.0)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (1.14.3)\n",
      "Requirement already satisfied: pytz in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (2020.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (1.52.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage>=1.13.0->kfp) (3.13.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.6.0->google-cloud-storage>=1.13.0->kfp) (2.20)\n",
      "Building wheels for collected packages: kfp, kfp-server-api, strip-hints, docstring-parser\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.1.2-py3-none-any.whl size=218827 sha256=56096c4385a3e1e3ee6af7965c06ea47c5f16e81fc5ba9f16e91646d918785e3\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/cc/e1/79/371d465a7b85585133d0a10a2aaf83aeaf7ff3c269d3d3cd12\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.1.2rc1-py3-none-any.whl size=108056 sha256=520c696b7ad4875b964a702f1897c2bfff0d26ab6c16dec501fc11e574e53f71\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/37/a8/7b/1b93439323d4182d4d5fff5e404c83aabf2fe25e430ec797b7\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20994 sha256=31987d438dfa265b58f60ed3c41d9630e4d9bf868c52601488694bd78ac82acc\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.7.3-py3-none-any.whl size=19230 sha256=a13ca0d3104430c13845a27e3d431aef9b426833470ce192212136f5e6041144\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ac/ed/39/ecb2e36c2893bb7b1324f6def66a7b3369c0bfc36ed2e07bb3\n",
      "Successfully built kfp kfp-server-api strip-hints docstring-parser\n",
      "Installing collected packages: kubernetes, requests-toolbelt, kfp-server-api, Deprecated, strip-hints, docstring-parser, kfp-pipeline-spec, kfp\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 12.0.0\n",
      "    Uninstalling kubernetes-12.0.0:\n",
      "      Successfully uninstalled kubernetes-12.0.0\n",
      "\u001b[31mERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n",
      "\n",
      "We recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n",
      "\n",
      "tfx 0.23.0 requires attrs<20,>=19.3.0, but you'll have attrs 20.3.0 which is incompatible.\n",
      "tfx 0.23.0 requires google-resumable-media<0.7.0,>=0.6.0, but you'll have google-resumable-media 1.1.0 which is incompatible.\n",
      "tfx 0.23.0 requires pyarrow<0.18,>=0.17, but you'll have pyarrow 2.0.0 which is incompatible.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.10 docstring-parser-0.7.3 kfp-1.1.2 kfp-pipeline-spec-0.1.3.1 kfp-server-api-1.1.2rc1 kubernetes-11.0.0 requests-toolbelt-0.9.1 strip-hints-0.1.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output directory\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9-okMV1ZFFsX"
   },
   "outputs": [],
   "source": [
    "#Preprocessing component\n",
    "def preprocess (data_path):\n",
    "  \n",
    "  # import libraries\n",
    "  import pickle\n",
    "  import os\n",
    "  import sys, subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "  subprocess.run([sys.executable, '-m', '!pip', 'install', 'scikit-learn==0.22'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'scipy'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.17.1'])\n",
    "    \n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "  # reading the dataset from the csv file\n",
    "  df_new = pd.read_csv(\"https://raw.githubusercontent.com/sophiabj/g05-used-cars/master/data/new_vehicle.csv\")\n",
    "  # selecting features, X\n",
    "  X = df_new.iloc[:, :-1].values\n",
    "  # selecting labels, y\n",
    "  y = df_new.iloc[:, -1].values\n",
    "\n",
    "  # normalize the data\n",
    "  X = StandardScaler().fit_transform(X.astype(float))\n",
    "\n",
    "  # to split the data\n",
    "  # split into train and test\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  #Save output file to path\n",
    "  np.savez_compressed(f'{data_path}/preprocessed-data.npz',\n",
    "                     X_train=X_train,\n",
    "                     X_test=X_test,\n",
    "                     y_train=y_train,\n",
    "                     y_test=y_test)\n",
    "  print(\"Done preprocessing..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done preprocessing..\n"
     ]
    }
   ],
   "source": [
    "preprocess(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training component\n",
    "def train(data_path):\n",
    "  import pickle\n",
    "  import sys \n",
    "  import os\n",
    "  \n",
    "  import subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'sklearn'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'scipy'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'cython'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'joblib'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.16.1'])\n",
    "\n",
    "  import joblib\n",
    "  import numpy as np\n",
    "  #from sklearn.tree import DecisionTreeRegressor\n",
    "  from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    " \n",
    "    \n",
    "  preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "\n",
    "  X_train = preprocessed_data['X_train']\n",
    "  y_train = preprocessed_data['y_train']\n",
    "\n",
    "  r = ExtraTreesRegressor(n_estimators=400, random_state=42)\n",
    "  r.fit(X_train, y_train.ravel())\n",
    "\n",
    "  with open(f'{data_path}/model', 'wb') as f:\n",
    "       pickle.dump(r, f)\n",
    "    \n",
    "  print(\"Done training\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n"
     ]
    }
   ],
   "source": [
    "train(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction component\n",
    "def predict (data_path):\n",
    "  import pickle\n",
    "  import os\n",
    "  import sys, subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn'])\n",
    "  import numpy as np\n",
    "  from sklearn.metrics import mean_absolute_error as MAE\n",
    "  from sklearn.metrics import mean_squared_error as MSE\n",
    "  from sklearn import metrics\n",
    "  from sklearn.metrics import r2_score\n",
    "  #from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "\n",
    "  #Load saved model\n",
    "  with open(f'{data_path}/model','rb') as file:\n",
    "    model = pickle.load(file)\n",
    "    \n",
    "  preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "  X_test = preprocessed_data['X_test']\n",
    "  y_test = preprocessed_data['y_test']\n",
    "\n",
    "\n",
    "  y_predET = model.predict(X_test)\n",
    "\n",
    "  print('Mean Absolute Error: ', round(metrics.mean_absolute_error(y_test, y_predET), 3))\n",
    "  print('Mean Squared Error: ', round(metrics.mean_squared_error(y_test, y_predET), 3))\n",
    "  #print('Root Mean Squared Error: ', round(np.sqrt(metrics.mean_squared_error(y_test, y_predET)), 3))\n",
    "  print('R2 score: ', round(r2_score(y_test, y_predET), 3))\n",
    "\n",
    "  #save result\n",
    "  with open(f'{data_path}/model_result', 'wb') as result:\n",
    "    pickle.dump(y_predET, result)\n",
    "    \n",
    "  \n",
    "  print(\"Prediction saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error:  1489.648\n",
      "Mean Squared Error:  8553142.44\n",
      "R2 score:  0.928\n",
      "Prediction saved!\n"
     ]
    }
   ],
   "source": [
    "predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packaging components\n",
    "preprocess_op = comp.func_to_container_op(preprocess , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_op = comp.func_to_container_op(train , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to kfp\n",
    "import kfp\n",
    "client = kfp.Client(host='6b708c366dbe3abd-dot-us-central2.pipelines.googleusercontent.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Used Cars Pipeline',\n",
    "   description='An ML pipeline that predicts price of used cars.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def used_cars_container_pipeline(\n",
    "    data_path: str,\n",
    "    model_file: str\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create road safety training component.\n",
    "    used_cars_preprocess_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    used_cars_training_container = train_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: used_cars_preprocess_container.pvolume})\n",
    "\n",
    "    # Create road safety prediction component.\n",
    "    used_cars_predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: used_cars_training_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    used_cars_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: used_cars_predict_container.pvolume},\n",
    "        arguments=['head', f'{data_path}/model_result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH='used_cars_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = used_cars_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/kfp/dsl/_container_op.py:1028: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://6b708c366dbe3abd-dot-us-central2.pipelines.googleusercontent.com/#/experiments/details/c6d6e08e-ae81-4b8d-8256-695a9c1f0382\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://6b708c366dbe3abd-dot-us-central2.pipelines.googleusercontent.com/#/runs/details/711b36c7-759f-4be4-909f-3dd4c2bdb2f7\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'used_cars_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Preprocessing and modelling",
   "provenance": []
  },
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
