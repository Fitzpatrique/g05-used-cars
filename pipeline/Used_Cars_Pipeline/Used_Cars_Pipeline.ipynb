{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/HamoyeHQ/g05-used-cars/blob/master/Preprocessing_and_modelling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output directory\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9-okMV1ZFFsX"
   },
   "outputs": [],
   "source": [
    "#Preprocessing component\n",
    "def preprocess (data_path):\n",
    "  \n",
    "  # import libraries\n",
    "  import pickle\n",
    "  import os\n",
    "  import sys, subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "  #subprocess.run([sys.executable, '-m', '!pip', 'install', 'scikit-learn==0.22'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.17.1'])\n",
    "    \n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  from sklearn.model_selection import train_test_split\n",
    "  from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "  # reading the dataset from the csv file\n",
    "  df_new = pd.read_csv('new_vehicle.csv')\n",
    "  # selecting features, X\n",
    "  X = df_new.iloc[:, :-1].values\n",
    "  # selecting labels, y\n",
    "  y = df_new.iloc[:, -1].values\n",
    "\n",
    "  # normalize the data\n",
    "  X = StandardScaler().fit_transform(X.astype(float))\n",
    "\n",
    "  # to split the data\n",
    "  # split into train and test\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "  #Save output file to path\n",
    "  np.savez_compressed(f'{data_path}/preprocessed-data.npz',\n",
    "                     X_train=xtrain,\n",
    "                     X_test=xtest,\n",
    "                     y_train=ytrain,\n",
    "                     y_test=ytest)\n",
    "  print(\"Done preprocessing..\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training component\n",
    "def train(data_path):\n",
    "  import pickle\n",
    "  import sys \n",
    "  import os\n",
    "  import subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pip==20.2.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.23.4'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.22'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'numpy==1.16.1'])\n",
    "\n",
    "  from sklearn import metrics\n",
    "  from sklearn.metrics import r2_score\n",
    "  from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "    \n",
    " \n",
    "    \n",
    "  preprocessed_data = np.load(f'{data_path}/preprocessed-data.npz')\n",
    "\n",
    "  X_train = preprocessed_data['xtrain']\n",
    "  y_train = preprocessed_data['ytrain']\n",
    "\n",
    "  r = ExtraTreesRegressor(n_estimators=400, random_state=42)\n",
    "  r.fit(X_train, y_train.ravel())\n",
    "\n",
    "  with open(f'{data_path}/model', 'wb') as f:\n",
    "       pickle.dump(r, f)\n",
    "    \n",
    "  print(\"Done training\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Prediction component\n",
    "def predict (data_path):\n",
    "  import pickle\n",
    "  import sys, subprocess;\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas', 'scikit-learn'])\n",
    "  subprocess.run([sys.executable, '-m', 'pip', 'install','fbprophet', 'Prophet', 'plotly', ])\n",
    "  from sklearn.metrics import mean_absolute_error as MAE\n",
    "  from sklearn.metrics import mean_squared_error as MSE\n",
    "  from sklearn import metrics\n",
    "  from sklearn.metrics import r2_score\n",
    "  from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "\n",
    "  #Load saved model\n",
    "  with open(f'{data_path}/model','rb') as file:\n",
    "    model = pickle.load(file)\n",
    "\n",
    "\n",
    "  y_predET = model.predict(X_test)\n",
    "\n",
    "  print('Mean Absolute Error: ', round(metrics.mean_absolute_error(y_test, y_predET), 3))\n",
    "  print('Mean Squared Error: ', round(metrics.mean_squared_error(y_test, y_predET), 3))\n",
    "  #print('Root Mean Squared Error: ', round(np.sqrt(metrics.mean_squared_error(y_test, y_predET)), 3))\n",
    "  print('R2 score: ', round(r2_score(y_test, y_predET), 3))\n",
    "\n",
    "  #save result\n",
    "  with open(f'{data_path}/model_result', 'wb') as result:\n",
    "    pickle.dump(y_predET, result)\n",
    "    \n",
    "  \n",
    "  print(\"Prediction saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packaging components\n",
    "preprocess_op = comp.func_to_container_op(preprocess , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_op = comp.func_to_container_op(train , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connecting to kfp\n",
    "import kfp\n",
    "client = kfp.Client(host='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Used Cars Pipeline',\n",
    "   description='An ML pipeline that predicts price of used cars.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def road_safety_container_pipeline(\n",
    "    data_path: str,\n",
    "    model_file: str\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create road safety training component.\n",
    "    used_cars_preprocess_container = preprocess_op(data_path).add_pvolumes({data_path: vop.volume})\n",
    "    \n",
    "    used_cars_training_container = train_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: used_cars_preprocess_container.pvolume})\n",
    "\n",
    "    # Create road safety prediction component.\n",
    "    used_cars_predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: used_cars_training_container.pvolume})\n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    used_cars_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: used_cars_predict_container.pvolume},\n",
    "        arguments=['head', f'{data_path}/model_result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/mnt'\n",
    "MODEL_PATH='used_cars_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = used_cars_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'used_cars_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH,\n",
    "             \"model_file\":MODEL_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func,  \n",
    "  '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Preprocessing and modelling",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
